{
  
    
        "post0": {
            "title": "Inverse Autoregressive (Normalizing) Flows using JAX: a MADE implementation",
            "content": "Inverse Autoregressive (Normalizing) Flows using JAX: a MADE implementation . Introduction . Recently I’ve dwelled in the world of approximate Bayesian inference, or more generally speaking: Variational inference. I find it interesting because it’s fast, which usually doesn’t go very much in accordance with the whole Bayesian thing. It achives this by stating the problem as an optimization one and exploiting the automatic differentiation libraries availiable. Essentially, we are interested in approximating a function (in the Bayesian case it is usually a posterior distribution) with another function with parameters $ phi$, our objective is to find $ phi$ that minimizes the distance or divergence between the two. A (much) better, detailed explanation can be found in this review paper or in this blog post, I also find the good old wikipedia page to be a great resource. . As in all there are tradeoffs, in this case the speed comes at the price of inexactness. Recent literature has focused (at least from what I’ve read) on improving the family of parametrized functions we use to approximate the distribution we are interested in (I’ll talk from a Bayesian perspective). What started from mean field stuff I don’t understand grew to the current vanguard of normalizing flows, a fairly simple concept that builds upon itself to increase its complexity, allowing us (theoretically) to express a pretty big class of density functions. For completness I will state that any density function with strictly positive values on its entire domain and with differentiable conditional probabilities can be built from a base (uniform/normal) distribution, see section 2.2 of this paper. The beauty of a simple yet effective method has attracted much attention and it seems to have become the weapon of choice for the Bayesian variational inference-er (at least when dealing with continuous variables). . However, it is still only an approximation. And you know we (ok, me and my Bayesian lads) care about that EXACT inference. Thus, I’ve had the great idea of supplementing good old Monte Carlo with an as-close-as-possible-to-gaussian approximation of the density I’m interested in using variational inference with normalizing flows. But first, I must master the flow. Hence this post. . Normalizing Flows . No point in explaining the details when you can get it from the very best: this is a great review paper on the subject covering theory and applications with great references, it goes well beyond the scope of variational inferece; this paper focuses on flows for variational inference; this blog post is similar to the latter paper while this one has more intution, detail and code. I will only write the gist of it, so we can agree on notation: we are interested on a density function $p(x) propto tilde{p}(x)$ for $x in mathbb{R}^d$, we approximate it by transforming a random variable $u$, i.e. flow, and correcting for the changes in density caused by the transformation, i.e. normalize, in other words we use variable transformations in the following sense . x=fϕ(u)foru∼q(u).x = f_{ phi}(u) quad for quad u sim q(u).x=fϕ​(u)foru∼q(u). . Notice that the function transforming the variable is parametrized by a vector $ phi$, our objective is to optimize this vector. The base density $q(u)$ is gaussianlly normal and it can be parametrzied by its own vector $ varphi$. However, we will assume that the transformation from a standard multivariate normal distribution to a multivariate normal with paramters $ varphi$ can be absorved by $f_{ phi}$, i.e. making $ varphi$ part of $ phi$, see section 2.3.2. This way we have an approximation of our density of interest . qϕ(x)=q(fϕ−1(x))∣det⁡dfϕdu(fϕ−1(x))∣−1,q_{ phi}(x) = q(f_{ phi}^{-1}(x)) left| det frac{df_{ phi}}{du}(f_{ phi}^{-1}(x)) right|^{-1},qϕ​(x)=q(fϕ−1​(x))∣∣∣∣∣​detdudfϕ​​(fϕ−1​(x))∣∣∣∣∣​−1, . where $ frac{df_{ phi}}{du}(f_{ phi}^{-1}(x))$ is the Jacobian matrix of $f_{ phi}(u)$ evaluated at $f_{ phi}^{-1}(x)$. Notice, for all of this to work, $f_{ phi}$ must be invertible and differentiable. . The fact that allows us to complicate this simple idea indefinetly is that for any amount of invertible and differentiable transformations $f_1,…,f_K$ their composition $f_K circ cdots circ f_1$ is also invertible and differentiable, and the determinant of the Jacobian matrix for the composition can be easily derived by the identity . det⁡d(fK∘⋯∘f1)du(u)=det⁡dfKdu((fK−1∘⋯∘f1)(u))⋯det⁡df1du(u). det frac{d(f_K circ cdots circ f_1)}{du}(u) = det frac{df_K}{du}((f_{K-1} circ cdots circ f_1)(u)) cdots det frac{df_1}{du}(u).detdud(fK​∘⋯∘f1​)​(u)=detdudfK​​((fK−1​∘⋯∘f1​)(u))⋯detdudf1​​(u). . Now that we know our approximation and the parameters it depends on, we can optimize them to make this new density as close as possible to $p(x)$. To do this we need a measure of distance or divergence between the two. The Kullback-Leibler divergence is the preferred measure in the variational inferece literature, the normalizing flows literature (for some generalizations of it see section 2.3.4) and fairly prevalent in the Bayesian nonparametric posterior consistency literature, so lets stick to that for now. It also lets us use Monte Carlo estiamtes of its integrals if we have access to samples from either $u$ or $x$, this will come in handy on the implementation. The Kullback-Leibler divergence we’ll use to optimize our parameters $ phi$ can be simplified to . KL(qϕ(x)∣∣p(x))=∫log⁡(qϕ(x)p(x))qϕ(x)dxKL(q_{ phi}(x) || p(x)) = int log left( frac{q_{ phi}(x)}{p(x)} right) q_{ phi}(x)dxKL(qϕ​(x)∣∣p(x))=∫log(p(x)qϕ​(x)​)qϕ​(x)dx =∫log⁡(q(u)∣det⁡dfϕdu∣−1p(fϕ(u)))q(u)du= int log left( frac{q(u) left| det frac{df_{ phi}}{du} right|^{-1}} {p(f_{ phi}(u))} right) q(u)du=∫log⎝⎜⎛​p(fϕ​(u))q(u)∣∣∣∣​detdudfϕ​​∣∣∣∣​−1​⎠⎟⎞​q(u)du =Eq[log⁡(q(u))−log⁡(∣det⁡dfϕdu∣)−log⁡(p(fϕ(u)))]= mathbb{E}_ {q} left[ log(q(u)) - log( left| det frac{df_{ phi}}{du} right|) - log(p(f_{ phi}(u))) right]=Eq​[log(q(u))−log(∣∣∣∣​detdudfϕ​​∣∣∣∣​)−log(p(fϕ​(u)))] ∝−Eq[log⁡(∣det⁡dfϕdu∣)+log⁡(p~(fϕ(u)))], propto - mathbb{E}_ {q} left[ log( left| det frac{df_{ phi}}{du} right|) + log( tilde{p}(f_{ phi}(u))) right],∝−Eq​[log(∣∣∣∣​detdudfϕ​​∣∣∣∣​)+log(p~​(fϕ​(u)))], . where the first equality is by definition, the second is proved in appendix A, and the proportionality is w.r.t. $ phi$. In the usual Bayesian inference set up, we know the function $p(x)$ up to a constant of proportionality but have no samples from it; setting up the divergence as above makes it practical for Bayesian analysis, assuming that $q(u)$ is chosen such that we can easily sample from it. . Inverse Autoregressive Flow . The main burden in deriving our approximate density is that of computing the determinant of the Jacobian matrix $ frac{df_{ phi}}{du}$ at some given value. Each transformation in the flow would require to compute and store a $d x d$ square matrix to then compute its determinant. Even for relativeley small $d$ this could be computationally expensive and for a large $d$ it might be imposible to store in memory. Solving this issue is the goal of the Inverse Autoregressive flows: the Jacobian matrix of this transformation is lower triangular, i.e. the determinant is the product of its diagonal. Again, learn it from somebody that is smarter than me: this is the paper that introduced it, this blog post goes straight to the point, while this one provides more detail and intution behind the idea. . The concept is simple, at each step of the flow we scale and shift each dimension of the input variable with parameters determined by previous dimensions of the input variable. Formally, given in input variable $u$ we derive the output variable $x$ of the same dimension such that . xi=(fϕ(u))i=ui−μi(u1:i−1,ϕ)σi(u1:i−1,ϕ)i=1,…,d,x_i = (f_{ phi}(u))_i = frac{u_i - mu_i(u_{1:i-1}, phi)}{ sigma_i(u_{1:i-1}, phi)} quad i = 1, dots,d,xi​=(fϕ​(u))i​=σi​(u1:i−1​,ϕ)ui​−μi​(u1:i−1​,ϕ)​i=1,…,d, . where $ sigma_1( phi)$ and $ mu_1( phi)$ depend only on the flow’s parameters. Notice that the product of the elements on the diagonal of $ frac{df_{ phi}}{du}$ is $ prod_{i=1}^d sigma_i(u_{1:i-1}, phi)^{-1}$, avoiding computing the whole matrix and finding its determinant. . Now, regarding the derivation of parameters $( sigma_i(u_{1:i-1}, phi), mu_i(u_{1:i-1}, phi))_{i=1}^d$ the possibilities are endless. Black box deep learning practices allow for complex distributions to be transformed in relativley few steps of the flow into gaussian distributions (or at least it seems to be that way empirically) and, since we don’t need its functional form for any other derivation of the flow (only its value given some fixed parameters), they are our best choice in exploiting the potential of these methods. To implement this efficiently and allowing for a general class of architectures we use Masked Autoencoders for Distribution Estimation (MADE). Here is the original paper and some blog posts. . The autoencoder background is irrelevant for this particular application, all that provides value here is the addition of “masks” to the standard neural network architecture which allows for autoregressive outputs. With the masked architecture we can choose the number of hidden layers $h geq 0$ and the activation functions for the output and each hidden layer $(g_i)_ {i=1}^{h+1} $ to define autoregressive means (or log scales) given input $u$ and letting $H_0(u) = u$, . Hi(u)=gi(bi+(Wi⊙Mi)Hi−1(u))i=1,…,hμ(u,ϕ)=gh+1(bh+1+(Wh+1⊙Mh+1)Hh(u))H_i(u) = g_i(b_i+(W_i odot M_i)H_{i-1}(u)) quad i=1, dots,h mu(u, phi) = g_{h+1}(b_{h+1}+(W_{h+1} odot M_{h+1})H_{h}(u))Hi​(u)=gi​(bi​+(Wi​⊙Mi​)Hi−1​(u))i=1,…,hμ(u,ϕ)=gh+1​(bh+1​+(Wh+1​⊙Mh+1​)Hh​(u)) . where $ odot$ indicates element wise product, $(M_i)_ {i=1}^{h+1}$ are $d x d$ matrices with elements $m_{i,j} in {0,1}$ which are always (for Inverse Autoregressive flows) upper traingular and $M_1$ has zeros also on its diagonal, $(b_i)_ {i=1}^{h+1}$ and $(W_i)_ {i=1}^{h+1}$ are $d x 1$ vectors and $d x d$ matrices respectivley and the parameters of our model, i.e. $(b_i, W_i)_ {i=1}^{h+1} subset phi$ for each mean and log scale in our flow. Notice that this architecture ensures its output has the autoregressive characteristics we are interested in to simplify the calculation of the Jacobian of our flow, i.e. $( mu(u, phi))_ i = mu_i(u_{1:i-1}, phi)$. . JAX code . JAX is a growing functional python library that is able to compile and perform automatic differentiation on GPUs and TPUs (brrrrrrrr). Their documentation provides a great introduction to their syntax and a cookbook that can come in handy, also try to have their API documentation always at hand. This blog post provides example code in JAX for normalizing flows, also get a feel for the mechanics of JAX with some of their examples. . import jax.numpy as jnp import jax.random as rand . The objective is to build and algorithm that composes any finite number $K$ of Inverse Autoregressive transformations into a flow $f_{ phi} = f_K circ cdots circ f_1$, minimizes the Kullback-Lieber divergence between our density of interest $p(x)$ and the approximation from the flow $q_{ phi}(x)$, and efficiently transforms input observations $u$ into approximate observations $x$ with density $p(x)$. . Step 1: Masked Neural Architecture . The JAX functional programming style makes the parameters the central part of the algorithm. We will essentially create a series of functions that together randomly initialize the parameters, calculate the Kullback-Leiber divergence as a function of the parameters $ phi$ (and other fixed values), differentiate it w.r.t. $ phi$ and use these derivatives to minimize. Hence, it’s important how we feed these parameters to our functions, taking into consideration the autoregressive restrictions we need to impose and the JAX interface. Using pytrees is probably a good idea, to keep JAX happy, then one autoregressive transformation with $h = 1$ hidden layer for its log scale and $h=0$ hidden layers for its mean would require the parameters in pytree fashion . [(W1⊙M1,b1),(W2⊙M2,b2)][ (W_1 odot M_1 , b_1) , (W_2 odot M_2 , b_2) ][(W1​⊙M1​,b1​),(W2​⊙M2​,b2​)] . for the log scale and . [(W1⊙M1,b1)][ (W_1 odot M_1 , b_1) ][(W1​⊙M1​,b1​)] . for the mean. To implement the masked restrictions in code we simply feed them as a list of row vectors, ignoring the zeros. Consider a dimension $d=2$ to keep things simple, then our paramters would have the form . [ ([ [w_{1,2}], [] ], [b_{1}, b_{2}]), ([ [w_{1,1}, w_{1,2}], [w_{2,2}] ], [b_{1}, b_{2}]) ] #log scale [ ([ [w_{1,2}], [] ], [b_{1}, b_{2}]) ] #mean . then inside the functions we simply add the necessary zeros and we can do our linear combinations, activations and repeat until the complexity creates an intelligent being. Notice that the first layer always has zeros in the diagonals. In the same way we input the activation functions as a list [activation_fun1, activation_fun2] (for log scale) or a single activation function on a list [activation_fun] which the function will understand to mean [activation_fun, ..., activation_fun, identity_fun] for $h geq 1$ ([activation_fun1, identity_fun] for log scale). . Thus, the (Masked) Autoregressive neural network function that calculates the mean or log scale paramters of the model: . def DenseAutoReg (z, parameters, activations): if len(activations) == 1 and len(parameters) &gt; 1: h = len(parameters) activations = [activations[0] for _ in range(h-1)] + [None] else: assert len(parameters) == len(activations) for j, (W, b) in enumerate(parameters): if j == 0: W = [jnp.concatenate([jnp.zeros(i+1), w]) for i, w in enumerate(W)] else: W = [jnp.concatenate([jnp.zeros(i), w]) for i, w in enumerate(W)] W = jnp.array(W) z = jnp.dot(z, W) + b if activations[j] is not None: z = activations[j](z) return z . Step 2: Inverse Autoregressive Flow . An inverse autoregressive transformation would require as input parameters and activation functions of for both the mean and log scale. . def InvAutoRegFlow (z, mu_param, log_sd_param, mu_act, log_sd_act): mu = DenseAutoReg(z, mu_param, mu_act) log_sd = DenseAutoReg(z, log_sd_param, log_sd_act) return (z - mu)/jnp.exp(log_sd) # return jnp.exp(log_sd)*z + mu . Then we repeat this transformation $K$ times, this time extending the pytree for the paramters to a list with $K$ elements [ (mean_parameters1, log_scale_parameters1), ..., (mean_parametersK, log_scale_parametersK)], same for the activation functions. The function will also allow for the ordering of the inputs to be reversed at each step of the flow, because I’m a sheep (section 4.1.1), and output the log of the product of the determinant of the Jacobian matrices of each Inverse Autoregressive layer of the flow. . def MakeFlow (z, parameters, activations, invert = True): log_det_jac = 0. for param, act in zip(parameters, activations): log_det_jac -= jnp.sum(DenseAutoReg(z, param[1], act[1])) # log_det_jac += jnp.sum(DenseAutoReg(z, param[1], act[1])) z = InvAutoRegFlow(z, *param, *act) if invert: z = z[::-1] if invert: z = z[::-1] return z, log_det_jac . Step 3: Minimizing the Kullback-Leiber divergence . The integral needed to compute the Kullback-Leiber divergence (or at least the part which concerns $ phi$) is approximated by Monte Carlo, assuming that we can generate random variables with density $q(u)$. . def MCKLDiv (Z, parameters, activations, log_target, invert = True): KL = 0. for z in Z: x, log_det_jac = MakeFlow(z, parameters, activations, invert) KL -= log_det_jac + log_target(x) return KL/(jnp.shape(Z)[0]) . The last fundamental function needed withing the JAX framework is one that (randomly) initializes the parameters of the model to their aforementioned pytree structure. To do this we use a slight modification of the variance_scaling function of jax.nn.initializers that outputs vectors of different sizes for a specific dimension and is used to initialize the (masked) row vectors of $W odot M$ and $b$. . def variance_scaling(scale, distribution, dtype=jnp.float32): def init(key, denominator, shape, dtype=dtype): variance = jnp.array(scale / denominator, dtype=dtype) if distribution == &quot;truncated_normal&quot;: # constant is stddev of standard normal truncated to (-2, 2) stddev = jnp.sqrt(variance) / jnp.array(.87962566103423978, dtype) return rand.truncated_normal(key, -2, 2, shape, dtype) * stddev elif distribution == &quot;normal&quot;: return rand.normal(key, shape, dtype) * jnp.sqrt(variance) elif distribution == &quot;uniform&quot;: return rand.uniform(key, shape, dtype, -1) * jnp.sqrt(3 * variance) else: raise ValueError(&quot;invalid distribution for variance scaling initializer&quot;) return init . Then, the initializer function randomly builds the pytrees for a given dimension d, number of repetitions of the Inverse Autoregressive transformation K, and pytree of length K with the number of hidden layers at each transformation of the form [ (#hidden_layes_mean, #hidden_layes_log_scale), ...]. . def init_rand_param (d, K, hidden_layers, seed = 123, rng = variance_scaling(1., &quot;truncated_normal&quot;)): assert K == len(hidden_layers) keys = [rand.PRNGKey(seed)] parameters = [] for f in range(K): keys = rand.split(keys[0], 1+d+1) mu_layers = [([rng(keys[i+1], d/1., (d-(i+1), )) for i in range(d)], rng(keys[d+1], d/1., (d, )))] keys = rand.split(keys[0], 1+d+1) log_sd_layers = [([rng(keys[i+1], d/1., (d-(i+1), )) for i in range(d)], rng(keys[d+1], d/1., (d, )))] for h in range(hidden_layers[f][0]): keys = rand.split(keys[0], 1+d+1) mu_layers.append(([rng(keys[i+1], d/1., (d-i, )) for i in range(d)], rng(keys[d+1], d/1., (d, )))) for h in range(hidden_layers[f][1]): keys = rand.split(keys[0], 1+d+1) log_sd_layers.append(([rng(keys[i+1], d/1., (d-i, )) for i in range(d)], rng(keys[d+1], d/1., (d, )))) parameters.append((mu_layers, log_sd_layers)) return parameters . And that is all, now we simply choose an optimization algorithm from jax.experimental import optimizers, given some function of interest posterior, choosing paramters K, activations, hidden_layers we define our loss function, . loss = lambda param, Z: MCKLDiv(Z, param, activations, posterior) . generate randomly Z from a standard multivariate normal, and, assuming we are using sgd (choose a step_size), optimize that bitch. . from jax import value_and_grad, jit opt_init, opt_update, get_params = optimizers.sgd(step_size) @jit def update (i, opt_state, Z): params = get_params(opt_state) KL, grad_params = value_and_grad(loss)(params, Z) return KL, opt_update(i, grad_params, opt_state) params = init_rand_param(d, K, hidden_layers, rng = variance_scaling(1., &quot;normal&quot;)) opt_state = opt_init(params) for i in range(num_batches): KL, opt_state = update(i, opt_state, Z) print(&quot;KL Divergence = {}&quot;.format(KL)) . Example . There is much to be said about the Monte Carlo approximation of the gradient used in optimization, here is a review paper, and about the different variations of gradient descent that could be used, here is a review blog post. This example is only meant to prove that the method described above works and that it can build complex neural architectures. The objective is to approximate a multivariate normal distribution with a non diagonal covariance matrix and a non zero mean. This will be achieved using an Inverse Autoregressive flow of two transformations ($K=2$), one hidden layer for both the mean and log scale at each transformation, log sigmoid activation function for the hidden layers in the first transformation and ELU in the second, and a standard multivariate normal distribution as a base measure. . Consider an objective posterior density for $x in mathbb{R}^2$ s.t. x∼N2([2−2],[2−1−12]),x sim mathcal{N}_2 left( begin{bmatrix} 2 -2 end{bmatrix}, begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix} right),x∼N2​([2−2​],[2−1​−12​]), i.e. transform into . import itertools from jax import value_and_grad, jit from jax.experimental import optimizers as optim import jax.numpy as jnp import jax.nn as jnn import jax.random as rand import jax.scipy as jsp hidden_layers = [(1, 1), (1, 1)] K = 2 activations = [([jnn.log_sigmoid], [jnn.log_sigmoid]), ([jnn.elu], [jnn.elu])] MC_n = 100 d = 2 mean = jnp.array([2, -2]) var = jnp.array([[2, -1], [-1, 2]]) posterior = lambda x: jsp.stats.multivariate_normal.logpdf(x, mean, var) loss = lambda param, Z: MCKLDiv(Z, param, activations, posterior) opt_init, opt_update, get_params = optim.adagrad(step_size = 0.1, momentum = 0.9) @jit def update (i, opt_state, Z): params = get_params(opt_state) KL, grad_params = value_and_grad(loss)(params, Z) return KL, opt_update(i, grad_params, opt_state) params = init_rand_param(d, K, hidden_layers, rng = variance_scaling(1., &quot;normal&quot;)) opt_state = opt_init(params) itercount = itertools.count() key = rand.PRNGKey(123) Z = rand.normal(key, (MC_n, d)) Z_ = Z count = 0 while True: ind, div, param = -1, 1e10, [] stop = False while not stop: i = next(itercount) - count KL, opt_state = update(i+count, opt_state, Z) print(&quot; rKL Divergence = {}&quot;.format(KL), end = &quot; r&quot;) if KL &lt; div: ind, div, param = i, KL, opt_state elif i &gt; 1000: stop = True count += i Z_ = jnp.concatenate([Z_, Z]) Z_div = loss(get_params(param), Z_) print(&quot; nKL batch = {} (at {}), KL all = {} n&quot;.format(div, ind+1, Z_div)) yn = input(&quot;write anything to stop&quot;) if yn != &#39;&#39;: break key, subkey = rand.split(key) Z = rand.normal(subkey, (MC_n, d)) . After a couple of epochs we get parameters that transform 1100 observations with mean [0.038823530.01266078] begin{bmatrix} 0.03882353 0.01266078 end{bmatrix}[0.038823530.01266078​] and covariance [0.9359905−0.00276474−0.002764740.9968354] begin{bmatrix} 0.9359905 &amp; -0.00276474 -0.00276474 &amp; 0.9968354 end{bmatrix}[0.9359905−0.00276474​−0.002764740.9968354​] to mean [2.234608−2.1736727] begin{bmatrix} 2.234608 -2.1736727 end{bmatrix}[2.234608−2.1736727​] and covariance [1.6226339−0.88107896−0.881078962.2406228] begin{bmatrix} 1.6226339 &amp; -0.88107896 -0.88107896 &amp; 2.2406228 end{bmatrix}[1.6226339−0.88107896​−0.881078962.2406228​]. Visually to :rocket: .",
            "url": "https://albcab.github.io/blog/variational%20inference/normalizing%20flows/jax/2021/02/11/Inverse-Auto-Flows.html",
            "relUrl": "/variational%20inference/normalizing%20flows/jax/2021/02/11/Inverse-Auto-Flows.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://albcab.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://albcab.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://albcab.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://albcab.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}